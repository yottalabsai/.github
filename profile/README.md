# Yotta Labs

> **The AI-native operating system for GPU-scale ML workloads.**

We make elastic GPU compute fast, accessible, and production-ready â€” so engineers can ship models, not manage infrastructure.

---

## What We Build

| Product | Description |
|---|---|
| **Compute Pods** | Instant-ready GPU environments on H100/200, B200/300 and beyond |
| **Launch Templates** | Pre-configured deployment templates for zero-friction project starts |
| **Elastic Deployment** | Auto-scaling inference and training across regions |
| **Model APIs** | Unified routing across model providers for cost and latency optimization |
| **Quantization Tools** | Compress models for faster inference with minimal accuracy loss |


---

## Open Source

### ğŸ BloomBee

Run large language models in **decentralized, heterogeneous environments** with computational offloading. Built for teams that need to push inference beyond centralized data centers.

â†’ [BloomBee GitHub Repo](https://github.com/ai-decentralized/BloomBee)

### âš¡ NeuronMM

A high-performance **matrix multiplication kernel** for LLM inference on AWS Trainium. Minimizes data movement across memory hierarchies, maximizes SRAM and compute engine utilization, and eliminates expensive matrix transpose operations. Achieves up to **2.22Ã— kernel-level speedup** and **2.49Ã— end-to-end LLM inference speedup** with a 4.78Ã— reduction in HBM-SBUF memory traffic.

â†’ [NeuronMM GitHub Repo](https://github.com/PASAUCMerced/NeuronMM)

### ğŸ”´ AMD Kernel

High-performance **distributed GPU kernels for AMD MI300X** accelerators, optimizing the primitives that power modern LLMs â€” all-to-all communication (MoE), GEMM-ReduceScatter (tensor parallelism), and AllGather-GEMM (distributed inference). Built with zero-copy IPC and XCD-aware scheduling across 8 compute dies.

â†’ [AMD Inference Kernels GitHub Repo](https://github.com/yottalabsai/yotta_amd_kernel)

---

## Why Yotta

- âš¡ **On-demand, elastic GPU compute** â€” scale from a single GPU to large clusters, instantly
- ğŸ”’ **SOC 2 compliant** â€” enterprise-grade security and compliance baked in
- ğŸŒ **Multi-region availability** â€” reliable uptime for production workloads
- ğŸ§© **Persistent storage** â€” state that survives across deployments
- ğŸ› ï¸ **Batteries included** â€” from quick-start pods to full ML orchestration pipelines

---

## Get Started

- ğŸŒ **[Yotta Labs](https://yottalabs.ai)**
- ğ• **[Yotta Labs](https://x.com/YottaLabs)**
- ğŸ’¬ **[Discord](https://discord.com/invite/Ypexx2rxt9)**
- ğŸ’¼ **[LinkedIn](https://www.linkedin.com/company/yotta-labs/)**

---

*Multi-silicon. Multi-cloud. One platform built for enterprise AI at any scale.*
